{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c15852-af32-41b8-8265-c945c3dba3ad",
   "metadata": {},
   "source": [
    "# Part2: Cleaning/Formatting Website Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35ac515-7d20-47ed-9051-ec7399f882f5",
   "metadata": {},
   "source": [
    "*Milestone Objective:* This milestone aims to perform data transformations and cleansing on website data related to U.S. States Populations, Land Area, and Population Density (https://www.states101.com/populations). The goal is to create a clean and usable dataset for further analysis by executing at least five specific data transformation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62058202-4f34-4bc8-ae8b-0786f987be6f",
   "metadata": {},
   "source": [
    "### Transformation Steps and Code Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3fcd89c-9b36-43cd-80ca-69280ad95523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        State  Population  Land Area (sq mi)  Density (people per sq mi)\n",
      "0  California    38803000          155779.03                       249.1\n",
      "1       Texas    26957000          261231.59                       103.2\n",
      "2     Florida    19893000           53624.55                       371.0\n",
      "3    New York    19746000           47126.45                       419.0\n",
      "4    Illinois    12881000           55518.76                       232.0\n"
     ]
    }
   ],
   "source": [
    "# Step #0: Scrape and Parse Data from Website into DataFrame\n",
    "\n",
    "# In this step, extracted the population data from the website using the requests and BeautifulSoup libraries.\n",
    "# fetched the HTML, identified the table containing population data, and converted it into a structured pandas DataFrame.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = 'https://www.states101.com/populations'\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200 means OK)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the webpage using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the table with the population data by ID ('populationTable')\n",
    "    table = soup.find('table', id='populationTable')\n",
    "    \n",
    "    # Find all the rows in the table, skipping the header row (index 1 onward)\n",
    "    rows = table.find_all('tr')[1:]   \n",
    "\n",
    "     # Initialize an empty list to hold the scraped data\n",
    "    data = []\n",
    "\n",
    "    # Iterate through the rows of the table\n",
    "    for row in rows:\n",
    "        # Extract all 'td' (table data) elements from the row\n",
    "        cols = row.find_all('td')\n",
    "\n",
    "         # Check if the number of columns is as expected (at least 4 columns)\n",
    "        if len(cols) < 4:\n",
    "            print(f\"Found {len(cols)} columns in row: {row}\")\n",
    "            continue # Skip the row if columns are fewer than expected\n",
    "\n",
    "        # Extract and clean the state name\n",
    "        state = cols[0].text.strip() # Strip any extra spaces or newlines\n",
    "\n",
    "        # Extract and clean the population value\n",
    "        # Remove commas, extra quotes, and newlines, then remove the rank (e.g., '(1st)')\n",
    "        population = cols[1].text.strip().split(' ')[0].replace(',', '').replace('\"', '').replace('\\n', '').strip()\n",
    "        population = population.split('(')[0].strip()  # Remove anything inside parentheses (rank)\n",
    "        population = int(population)  # Convert the cleaned population value to an integer\n",
    "\n",
    "        # Extract and clean the land area value\n",
    "        # Similar cleaning steps for land area as for population\n",
    "        land_area = cols[2].text.strip().split(' ')[0].replace(',', '').replace('\"', '').replace('\\n', '').strip()\n",
    "        land_area = land_area.split('(')[0].strip()  # Remove rank\n",
    "        land_area = float(land_area)  # Convert to float\n",
    "\n",
    "        # Extract and clean the density value\n",
    "        # Again, apply similar cleaning to remove unnecessary characters and rank\n",
    "        density = cols[3].text.strip().split(' ')[0].replace(',', '').replace('\"', '').replace('\\n', '').strip()\n",
    "        density = density.split('(')[0].strip()  # Remove rank\n",
    "        density = float(density)   # Convert to float for numerical calculations\n",
    "\n",
    "        # Append the cleaned data (state, population, land area, density) to the list\n",
    "        data.append([state, population, land_area, density])\n",
    "\n",
    "    # Create a DataFrame from the cleaned list of data\n",
    "    df = pd.DataFrame(data, columns=['State', 'Population', 'Land Area (sq mi)', 'Density (people per sq mi)'])\n",
    "\n",
    "    # Display the first few rows of the DataFrame to ensure data is loaded correctly\n",
    "    print(df.head())\n",
    "\n",
    "else:\n",
    "     # If the request was unsuccessful, print the status code (e.g., 404, 500)\n",
    "    print(f\"Failed to retrieve data: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b0f519-c71c-4643-9082-76747169ebdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['state', 'population', 'land_area_(sq_mi)',\n",
      "       'density_(people_per_sq_mi)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Step #1: Standardize column names for consistency\n",
    "\n",
    "# This step converts all column names to lowercase and replaces spaces with underscores.\n",
    "# This makes it easier to reference the columns later in the analysis and ensures consistency.\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Display the updated column names to verify the change\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f6e50a3-92c4-4d22-bc2f-17fadc056547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        state  population  land_area_(sq_mi)  density_(people_per_sq_mi)\n",
      "0  California    38803000          155779.03                       249.1\n",
      "1       Texas    26957000          261231.59                       103.2\n",
      "2     Florida    19893000           53624.55                       371.0\n",
      "3    New York    19746000           47126.45                       419.0\n",
      "4    Illinois    12881000           55518.76                       232.0\n"
     ]
    }
   ],
   "source": [
    "# Step #2: Fix casing for state names\n",
    "\n",
    "# Normalizing the casing of state names ensures uniformity, which is essential for tasks like mapping state names to their codes and avoiding errors in comparisons later.\n",
    "df['state'] = df['state'].str.title()\n",
    "\n",
    "# Display the DataFrame to verify the change\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ef74206-bc29-4814-ab84-ff569f8c6bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows removed: 0\n",
      "        state  population  land_area_(sq_mi)  density_(people_per_sq_mi)\n",
      "0  California    38803000          155779.03                       249.1\n",
      "1       Texas    26957000          261231.59                       103.2\n",
      "2     Florida    19893000           53624.55                       371.0\n",
      "3    New York    19746000           47126.45                       419.0\n",
      "4    Illinois    12881000           55518.76                       232.0\n"
     ]
    }
   ],
   "source": [
    "# Step #3: Identify and remove duplicate entries\n",
    "\n",
    "# Removing duplicates early in the process prevents them from affecting our analysis. \n",
    "# This step ensures that each record in our dataset is unique before we perform more complex transformations.\n",
    "\n",
    "# Count the number of rows before removing duplicates\n",
    "initial_count = df.shape[0]\n",
    "\n",
    "# Remove duplicates based on all columns\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Count the number of rows after removing duplicates\n",
    "final_count = df.shape[0]\n",
    "\n",
    "# Display the number of duplicates removed\n",
    "print(f\"Number of duplicate rows removed: {initial_count - final_count}\")\n",
    "\n",
    "# Display the DataFrame to verify the change\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05fdb41c-97f6-47a3-bb75-648e66bc25b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        state  population  land_area_(sq_mi)  density_(people_per_sq_mi)  \\\n",
      "0  California    38803000          155779.03                       249.1   \n",
      "1       Texas    26957000          261231.59                       103.2   \n",
      "2     Florida    19893000           53624.55                       371.0   \n",
      "3    New York    19746000           47126.45                       419.0   \n",
      "4    Illinois    12881000           55518.76                       232.0   \n",
      "\n",
      "  state_code  \n",
      "0         CA  \n",
      "1         TX  \n",
      "2         FL  \n",
      "3         NY  \n",
      "4         IL  \n"
     ]
    }
   ],
   "source": [
    "# Step #4: Create a State Code Mapping Using the Dictionary\n",
    "\n",
    "# With clean state names in a consistent format, we can now reliably map them to their corresponding state codes.\n",
    "# Did this to ensure consistency across all 3 data sources (i.e., the flat file, website data, and API),\n",
    "# as the state code will be the key used for joining in the later analysis.\n",
    "\n",
    "# Mapping of full state names to their two-character codes\n",
    "state_mapping = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\"\n",
    "}\n",
    "\n",
    "# Apply the state mapping to create a new column with state codes\n",
    "df['state_code'] = df['state'].map(state_mapping)\n",
    "\n",
    "# Display the updated DataFrame to ensure that state codes have been added correctly\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50c01e04-39be-4a8c-98a5-9b1edf0562d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count before filtering: 56 rows\n",
      "Count after filtering: 50 rows\n",
      "        state  population  land_area_(sq_mi)  density_(people_per_sq_mi)  \\\n",
      "0  California    38803000          155779.03                       249.1   \n",
      "1       Texas    26957000          261231.59                       103.2   \n",
      "2     Florida    19893000           53624.55                       371.0   \n",
      "3    New York    19746000           47126.45                       419.0   \n",
      "4    Illinois    12881000           55518.76                       232.0   \n",
      "\n",
      "  state_code  \n",
      "0         CA  \n",
      "1         TX  \n",
      "2         FL  \n",
      "3         NY  \n",
      "4         IL  \n"
     ]
    }
   ],
   "source": [
    "# Step #5: Filter out any non-U.S. states\n",
    "\n",
    "# After ensuring the data is unique and consistently formatted, we can now filter out any non-U.S. states.\n",
    "# This is important as non-U.S. states or territories could be present in the data and would skew analysis if not removed.\n",
    "\n",
    "# Display the count before filtering\n",
    "count_before_filter = df.shape[0]\n",
    "print(f\"Count before filtering: {count_before_filter} rows\")\n",
    "\n",
    "# Filter out any non-U.S. states by checking for non-null values in the 'state_code' column.\n",
    "df = df[df['state_code'].notna()]\n",
    "\n",
    "# Display the count after filtering to see how many rows remain\n",
    "count_after_filter = df.shape[0]\n",
    "print(f\"Count after filtering: {count_after_filter} rows\")\n",
    "\n",
    "# Final Output: Print the cleaned DataFrame to verify the result\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e224b9-2405-48d6-b861-07931e973f1c",
   "metadata": {},
   "source": [
    "### Ethical Implications of Data Wrangling from the Website Data \n",
    "*https://www.states101.com/populations*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aeb995-9358-42e8-932c-11e5da258e88",
   "metadata": {},
   "source": [
    "In the process of cleaning and transforming the population data from the website, several steps were applied, such as standardizing column names, normalizing the casing of state names, removing duplicate entries, mapping state names to two-letter codes, and filtering out non-U.S. states. These changes enhanced the dataset for analysis, but they also come with ethical considerations. The data was sourced from the U.S. Census Bureau via Census.gov, which is a credible and authoritative source. However, legal and regulatory guidelines regarding web scraping must still be considered, as terms of service agreements may restrict such activities. The transformations, such as excluding non-U.S. states, could introduce risks by potentially omitting important data. Additionally, assumptions made during data cleaning, such as the consistency of state names, could lead to inaccuracies. Since the data was sourced from a credible government site, its accuracy is more likely, but it’s important to respect any restrictions on data use. To mitigate these ethical risks, careful consideration of the terms of use, clear documentation of assumptions, and ongoing review of the data’s integrity are necessary to ensure compliance and reliability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
